---
title: homepage
date: 2020-10-17 01:24:19
tags:
academia: true
---
# About

I am a third-year Ph.D. student researching Natural Language Processing at the University of Waterloo. I work with [Ming Li](https://scholar.google.ca/citations?user=oGgPXFEAAAAJ&hl=en) on language modeling and unsupervised methods (See CV for details).

In general, my research investigates how to represent languages for computing. Lately, I am obsessed with language modeling which represents language via neural computing for its unsupervised and task-agnostic nature.

My thesis concerns modeling text and speech sequences to achieve lower perplexity, better generation, and benefit downstream language tasks; specifically, we address the problem of modeling text and text-speech sequences with Transformer-based language models. In the past two years, I have proposed three new techniques that improve sequence modeling in different ways: [Segment-Aware Language Modeling](https://arxiv.org/abs/2004.14996), [Hypernym-Instructed Language Modeling](arxiv.org/abs/2203.10692), and [Alignment-Aware Acoustic and Text Modeling](arxiv.org/abs/2203.09690). I am also interested in multilingual NLP.

# Publications
**He Bai**, Renjie Zheng, Junkun Chen, Xintong Li, Mingbo Ma, Liang Huang. A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing.  ICML 2022 (full paper) [[pdf]](https://arxiv.org/abs/2203.09690)[codes released after camera-ready].

**He Bai**, Tong Wang, Alessandro Sordoni, Peng Shi. Better Language Model with Hypernym Class Prediction. ACL 2022 (full paper) [[pdf]](https://openreview.net/pdf?id=YjZH6EpuSY) [[codes]](https://github.com/richardbaihe/robustLM).

Peng Shi, Rui Zhang, **He Bai**, Jimmy Lin. Cross-Lingual Training with Dense Retrieval for Document Retrieval. EMNLP-MSR 2021 (workshop paper) [[pdf]](https://arxiv.org/pdf/2109.01628.pdf).

**He Bai**, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu, Ming Li. Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation. ACL-SRW 2021 (workshop paper) [[pdf]](https://arxiv.org/pdf/2004.02251.pdf) [[codes]](https://github.com/rsvp-ai/semantic_unwritten).

**He Bai**, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li. Segatron: Segment-awareTransformer for Language Modeling and Understanding. AAAI 2021. (full paper) [[pdf]](https://arxiv.org/abs/2004.14996) [[codes]](https://github.com/rsvp-ai/segatron_aaai)

Peng Shi, **He Bai**, Jimmy Lin. Cross-Lingual Training of Neural Models for Document Ranking. EMNLP Findings 2020. (short paper) [[pdf]](https://www.aclweb.org/anthology/2020.findings-emnlp.249/) [[codes]](https://github.com/Impavidity/cross-lingual-doc-ranking)


**He Bai**, Yu Zhou, Jiajun Zhang and Chengqing Zong. Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference. ACL 2019. (short paper) [[pdf]](https://arxiv.org/pdf/1906.01788.pdf) [[codes]](https://github.com/richardbaihe/conslu)


**He Bai**, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh Hwang and Chengqing Zong. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language. COLING 2018. (full paper) [[pdf]](https://arxiv.org/pdf/1808.06167.pdf) 





